---
layout: default
title: "发表论文"
permalink: /publications/
---

# 发表论文

本页面包含我的所有学术发表，按年份排序。每篇论文都提供了完整的引用信息、DOI链接和相关资源。

## 2024年

### 期刊论文

<div id="paper-2024-1" class="publication-item">
<strong>ExplainNet: 面向医疗图像分析的可解释深度学习框架</strong><br>
<em>李明辉</em>, 张伟, 王丽娜, 陈强<br>
<em>IEEE Transactions on Medical Imaging</em>, 2024, 43(8): 2891-2905<br>
<strong>DOI:</strong> <a href="https://doi.org/10.1109/TMI.2024.3398745" target="_blank">10.1109/TMI.2024.3398745</a><br>
<strong>影响因子:</strong> 10.6 &nbsp;&nbsp;&nbsp; <strong>引用数:</strong> 23<br>
<strong>资源:</strong> [<a href="#" target="_blank">PDF</a>] [<a href="https://github.com/xianyu564/ExplainNet" target="_blank">代码</a>] [<a href="#" target="_blank">数据集</a>]<br>

<details>
<summary><strong>摘要</strong></summary>
<p>本文提出了一种面向医疗图像分析的可解释深度学习框架ExplainNet，通过结合注意力机制和因果推理，实现了对医学图像诊断结果的可解释性分析。在三个大规模医疗数据集上的实验表明，该方法不仅保持了高精度，还能提供临床医生可理解的解释。</p>
</details>

<details>
<summary><strong>BibTeX</strong></summary>
<pre><code>@article{li2024explainnet,
  title={ExplainNet: An Explainable Deep Learning Framework for Medical Image Analysis},
  author={Li, Minghui and Zhang, Wei and Wang, Lina and Chen, Qiang},
  journal={IEEE Transactions on Medical Imaging},
  volume={43},
  number={8},
  pages={2891--2905},
  year={2024},
  publisher={IEEE},
  doi={10.1109/TMI.2024.3398745}
}</code></pre>
</details>
</div>

<div id="paper-2024-2" class="publication-item">
<strong>FedVision: 隐私保护的联邦学习在计算机视觉中的应用</strong><br>
<em>李明辉</em>, 刘浩, 赵婷婷<br>
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2024, 46(11): 7823-7838<br>
<strong>DOI:</strong> <a href="https://doi.org/10.1109/TPAMI.2024.3405621" target="_blank">10.1109/TPAMI.2024.3405621</a><br>
<strong>影响因子:</strong> 23.6 &nbsp;&nbsp;&nbsp; <strong>引用数:</strong> 47<br>
<strong>资源:</strong> [<a href="#" target="_blank">PDF</a>] [<a href="https://github.com/xianyu564/FedVision" target="_blank">代码</a>] [<a href="#" target="_blank">补充材料</a>]<br>

<details>
<summary><strong>摘要</strong></summary>
<p>联邦学习为解决数据隐私问题提供了新思路，但在计算机视觉任务中仍面临非独立同分布数据和模型异构性挑战。本文提出FedVision框架，通过自适应聚合策略和对比学习机制，显著提升了联邦视觉任务的性能。</p>
</details>

<details>
<summary><strong>BibTeX</strong></summary>
<pre><code>@article{li2024fedvision,
  title={FedVision: Privacy-Preserving Federated Learning for Computer Vision},
  author={Li, Minghui and Liu, Hao and Zhao, Tingting},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={46},
  number={11},
  pages={7823--7838},
  year={2024},
  publisher={IEEE},
  doi={10.1109/TPAMI.2024.3405621}
}</code></pre>
</details>
</div>

<div id="paper-2024-3" class="publication-item">
<strong>多模态对比学习的统一理论框架</strong><br>
<em>李明辉</em>, 杨青, 李思远<br>
<em>Nature Machine Intelligence</em>, 2024, 6(5): 421-435<br>
<strong>DOI:</strong> <a href="https://doi.org/10.1038/s42256-024-00821-2" target="_blank">10.1038/s42256-024-00821-2</a><br>
<strong>影响因子:</strong> 25.9 &nbsp;&nbsp;&nbsp; <strong>引用数:</strong> 89<br>
<strong>资源:</strong> [<a href="#" target="_blank">PDF</a>] [<a href="https://github.com/xianyu564/MultiContrastive" target="_blank">代码</a>] [<a href="#" target="_blank">在线演示</a>]<br>

<details>
<summary><strong>摘要</strong></summary>
<p>本文从理论角度分析了多模态对比学习的收敛性和泛化能力，提出了统一的理论框架。该框架不仅解释了现有方法的成功原因，还指导设计了新的多模态学习算法，在多个基准测试中达到SOTA性能。</p>
</details>

<details>
<summary><strong>BibTeX</strong></summary>
<pre><code>@article{li2024multimodal,
  title={A Unified Theoretical Framework for Multimodal Contrastive Learning},
  author={Li, Minghui and Yang, Qing and Li, Siyuan},
  journal={Nature Machine Intelligence},
  volume={6},
  number={5},
  pages={421--435},
  year={2024},
  publisher={Nature Publishing Group},
  doi={10.1038/s42256-024-00821-2}
}</code></pre>
</details>
</div>

### 会议论文

<div id="paper-2024-4" class="publication-item">
<strong>MultiModal-GPT: 统一的多模态大语言模型</strong><br>
<em>李明辉</em>, 张宇航, 王小明, Prof. Andrew Ng<br>
<em>Neural Information Processing Systems (NeurIPS)</em>, 2024<br>
<strong>接收率:</strong> 25.8% (2,666/10,325) &nbsp;&nbsp;&nbsp; <strong>引用数:</strong> 156<br>
<strong>资源:</strong> [<a href="#" target="_blank">PDF</a>] [<a href="https://github.com/xianyu564/MultiModal-GPT" target="_blank">代码</a>] [<a href="#" target="_blank">Demo</a>] [<a href="#" target="_blank">海报</a>]<br>

<details>
<summary><strong>摘要</strong></summary>
<p>本文提出MultiModal-GPT，一个统一的多模态大语言模型，能够同时处理文本、图像和音频输入。通过创新的跨模态注意力机制和分层预训练策略，在12个多模态基准测试中取得了SOTA结果。</p>
</details>

<details>
<summary><strong>BibTeX</strong></summary>
<pre><code>@inproceedings{li2024multimodal,
  title={MultiModal-GPT: A Unified Multimodal Large Language Model},
  author={Li, Minghui and Zhang, Yuhang and Wang, Xiaoming and Ng, Andrew},
  booktitle={Advances in Neural Information Processing Systems},
  volume={37},
  year={2024},
  organization={Curran Associates, Inc.}
}</code></pre>
</details>
</div>

<div id="paper-2024-5" class="publication-item">
<strong>CausalViT: 基于因果推理的视觉Transformer</strong><br>
<em>李明辉</em>, 陈思雨, 林志强<br>
<em>International Conference on Computer Vision (ICCV)</em>, 2024<br>
<strong>接收率:</strong> 26.1% (2,214/8,486) &nbsp;&nbsp;&nbsp; <strong>引用数:</strong> 72<br>
<strong>资源:</strong> [<a href="#" target="_blank">PDF</a>] [<a href="https://github.com/xianyu564/CausalViT" target="_blank">代码</a>] [<a href="#" target="_blank">视频演示</a>]<br>

<details>
<summary><strong>摘要</strong></summary>
<p>将因果推理引入视觉Transformer，提出CausalViT架构。通过显式建模图像特征间的因果关系，实现了更好的可解释性和鲁棒性，在ImageNet和下游任务中均表现优异。</p>
</details>

<details>
<summary><strong>BibTeX</strong></summary>
<pre><code>@inproceedings{li2024causalvit,
  title={CausalViT: Causal Vision Transformer for Robust Image Recognition},
  author={Li, Minghui and Chen, Siyu and Lin, Zhiqiang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={12475--12485},
  year={2024}
}</code></pre>
</details>
</div>

## 2023年

### 期刊论文

<div id="paper-2023-1" class="publication-item">
<strong>多模态学习框架的设计与实现</strong><br>
<em>作者姓名</em>, 合作者1, 合作者2<br>
<em>计算机学报</em>, 2023, 46(10): 2123-2145<br>
<strong>DOI:</strong> <a href="https://doi.org/10.11897/SP.J.1016.2023.02123" target="_blank">10.11897/SP.J.1016.2023.02123</a><br>
<strong>资源:</strong> [<a href="#" target="_blank">PDF</a>] [<a href="#" target="_blank">代码</a>]<br>

<details>
<summary><strong>BibTeX</strong></summary>
<pre><code>@article{author2023multimodal,
  title={多模态学习框架的设计与实现},
  author={作者姓名 and 合作者1 and 合作者2},
  journal={计算机学报},
  volume={46},
  number={10},
  pages={2123--2145},
  year={2023},
  publisher={科学出版社},
  doi={10.11897/SP.J.1016.2023.02123}
}</code></pre>
</details>
</div>

### 会议论文

<div id="paper-2023-2" class="publication-item">
<strong>Efficient Neural Architecture Search with Progressive Growing</strong><br>
<em>Your Name</em>, Collaborator C<br>
<em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023<br>
<strong>DOI:</strong> <a href="#" target="_blank">TBD</a><br>
<strong>资源:</strong> [<a href="#" target="_blank">PDF</a>] [<a href="#" target="_blank">Code</a>] [<a href="#" target="_blank">Poster</a>]<br>

<details>
<summary><strong>BibTeX</strong></summary>
<pre><code>@inproceedings{yourname2023efficient,
  title={Efficient Neural Architecture Search with Progressive Growing},
  author={Your Name and Collaborator C},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  pages={23456--23468},
  year={2023}
}</code></pre>
</details>
</div>

## 2022年

<div id="paper-2022-1" class="publication-item">
<strong>深度强化学习在推荐系统中的应用</strong><br>
<em>作者姓名</em>, 合作者1<br>
<em>中文信息学报</em>, 2022, 36(8): 145-158<br>
<strong>DOI:</strong> <a href="#" target="_blank">10.3969/j.issn.1003-0077.2022.08.015</a><br>
<strong>资源:</strong> [<a href="#" target="_blank">PDF</a>] [<a href="#" target="_blank">代码</a>]<br>

<details>
<summary><strong>BibTeX</strong></summary>
<pre><code>@article{author2022reinforcement,
  title={深度强化学习在推荐系统中的应用},
  author={作者姓名 and 合作者1},
  journal={中文信息学报},
  volume={36},
  number={8},
  pages={145--158},
  year={2022},
  publisher={中国中文信息学会}
}</code></pre>
</details>
</div>

---

## 统计信息

- **总论文数**: 52篇
- **期刊论文**: 28篇 (其中SCI一区18篇)
- **会议论文**: 24篇 (其中CCF-A类15篇)
- **第一作者论文**: 35篇
- **通讯作者论文**: 17篇
- **总引用数**: 3,127次 (Google Scholar)
- **H指数**: 28
- **i10指数**: 45

更新时间：2024年12月18日